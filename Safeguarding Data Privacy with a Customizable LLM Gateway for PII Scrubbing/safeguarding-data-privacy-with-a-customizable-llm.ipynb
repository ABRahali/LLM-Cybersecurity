{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T06:28:25.014277Z",
     "iopub.status.busy": "2024-12-26T06:28:25.013762Z",
     "iopub.status.idle": "2024-12-26T06:28:30.001222Z",
     "shell.execute_reply": "2024-12-26T06:28:29.999827Z",
     "shell.execute_reply.started": "2024-12-26T06:28:25.014234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: presidio-analyzer in /usr/local/lib/python3.10/dist-packages (2.2.356)\n",
      "Collecting presidio-anonymizer\n",
      "  Downloading presidio_anonymizer-2.2.356-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: phonenumbers<9.0.0,>=8.12 in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (8.13.52)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (6.0.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (2024.9.11)\n",
      "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (3.7.6)\n",
      "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer) (5.1.3)\n",
      "Collecting azure-core (from presidio-anonymizer)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: pycryptodome>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from presidio-anonymizer) (3.21.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (71.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.26.4)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core->presidio-anonymizer) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-core->presidio-anonymizer) (4.12.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio-analyzer) (3.10)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio-analyzer) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->presidio-analyzer) (3.16.1)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio-analyzer) (0.1.2)\n",
      "Downloading presidio_anonymizer-2.2.356-py3-none-any.whl (31 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: azure-core, presidio-anonymizer\n",
      "Successfully installed azure-core-1.32.0 presidio-anonymizer-2.2.356\n"
     ]
    }
   ],
   "source": [
    "!pip install presidio-analyzer presidio-anonymizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2024-12-26T06:23:58.094503Z",
     "iopub.status.idle": "2024-12-26T06:23:58.094963Z",
     "shell.execute_reply": "2024-12-26T06:23:58.094783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T06:33:05.978233Z",
     "iopub.status.busy": "2024-12-26T06:33:05.977760Z",
     "iopub.status.idle": "2024-12-26T06:33:52.580005Z",
     "shell.execute_reply": "2024-12-26T06:33:52.578676Z",
     "shell.execute_reply.started": "2024-12-26T06:33:05.978200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d361b04c3f145739d207f2a76de28d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697719905a5644d18aea08cfa6fd8b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b038b21f0153415782fb1321f806563b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187c554b58524e14943b03a9c6c90812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c69da27ebb48769e47e18ac46ef67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8c7b94fa994ba383a723eb4f563837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 50, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [{'summary_text': 'Michael Smith committed a mistake when he used PyTorch Trainer instead of HF Trainer. He used the wrong name for the training program.'}]\n",
      "Detected PII: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdf9a94f7b4463b866fdc7013325a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d523aa02034782af027a606e198286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english_pii_43k.jsonl:   0%|          | 0.00/73.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1e3e6d342248e392957b760e625176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french_pii_62k.jsonl:   0%|          | 0.00/116M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094a43ecef8a4cc3ab9675fb1b6c1e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german_pii_52k.jsonl:   0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a8fa2e20b449a4870fb70a17328f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian_pii_50k.jsonl:   0%|          | 0.00/93.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6a696856e7403fac790a73721c36d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/209261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [{'summary_text': \"I need the latest update on assessment results. Please send the files to [REDACTED EMAIL ADDRESS]. For your extra time, we'll offer you Kip[REDACTED PHONE NUMBER]. But please provide your л\"}]\n",
      "Detected PII: []\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Set your environment variables (API keys, if needed, for cloud models)\n",
    "# For Hugging Face models, you generally don’t need an API key unless using specific models on Hugging Face Hub\n",
    "\n",
    "# Step 1: Load Hugging Face's NER pipeline\n",
    "# We will use a pretrained BERT model for Named Entity Recognition (NER)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Step 2: Define scrubbers for different types of PII\n",
    "def scrub_phone_numbers(text: str) -> str:\n",
    "    return re.sub(r\"\\(?\\+?[0-9]*\\)?[-.\\s]?[0-9]+[-.\\s]?[0-9]+[-.\\s]?[0-9]+\", \"[REDACTED PHONE NUMBER]\", text)\n",
    "\n",
    "def scrub_credit_card_numbers(text: str) -> str:\n",
    "    return re.sub(r\"\\b(?:\\d[ -]*?){13,16}\\b\", \"[REDACTED CREDIT CARD]\", text)\n",
    "\n",
    "def scrub_email_addresses(text: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"[REDACTED EMAIL ADDRESS]\", text)\n",
    "\n",
    "def scrub_postal_codes(text: str) -> str:\n",
    "    return re.sub(r\"\\b\\d{5}(-\\d{4})?\\b\", \"[REDACTED POSTAL CODE]\", text)\n",
    "\n",
    "def scrub_sin_numbers(text: str) -> str:\n",
    "    return re.sub(r\"\\b\\d{3}-\\d{3}-\\d{3}\\b\", \"[REDACTED SIN]\", text)\n",
    "\n",
    "# Combine all scrubbers into a list\n",
    "ALL_SCRUBBERS = [\n",
    "    scrub_phone_numbers,\n",
    "    scrub_credit_card_numbers,\n",
    "    scrub_email_addresses,\n",
    "    scrub_postal_codes,\n",
    "    scrub_sin_numbers,\n",
    "]\n",
    "\n",
    "# Step 3: Function to apply all scrubbers to input text\n",
    "def apply_scrubbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply all predefined scrubbers to the input text.\n",
    "    \"\"\"\n",
    "    for scrubber in ALL_SCRUBBERS:\n",
    "        text = scrubber(text)\n",
    "    return text\n",
    "\n",
    "# Step 4: Function to detect PII using Hugging Face's NER pipeline\n",
    "def detect_pii_with_huggingface(text: str):\n",
    "    \"\"\"\n",
    "    This function uses Hugging Face's NER model to detect PII in the text.\n",
    "    \"\"\"\n",
    "    # Detect named entities using Hugging Face's NER pipeline\n",
    "    ner_results = ner_pipeline(text)\n",
    "    \n",
    "    # Filter out non-PII entities (e.g., labels other than 'PER' for persons, 'ORG' for organizations)\n",
    "    pii_entities = [entity for entity in ner_results if entity['entity'] in ['PER', 'LOC', 'ORG', 'MISC']]\n",
    "    \n",
    "    return pii_entities\n",
    "\n",
    "# Step 5: Set up a function to send a request to a Hugging Face model (e.g., summarization or generation)\n",
    "def send_huggingface_request(text: str, model=\"facebook/bart-large-cnn\", max_tokens=50):\n",
    "    \"\"\"\n",
    "    Sends a request to a Hugging Face model after scrubbing PII.\n",
    "    \"\"\"\n",
    "    # Step 5.1: Detect and scrub PII using Hugging Face's NER model\n",
    "    detected_pii = detect_pii_with_huggingface(text)\n",
    "    scrubbed_text = apply_scrubbers(text)\n",
    "    \n",
    "    # Step 5.2: Load the Hugging Face model (for text summarization or generation)\n",
    "    summarizer = pipeline(\"summarization\", model=model)\n",
    "    \n",
    "    # Step 5.3: Generate a summary of the scrubbed text\n",
    "    summary = summarizer(scrubbed_text, max_length=max_tokens, min_length=25, do_sample=False)\n",
    "    \n",
    "    return summary, detected_pii\n",
    "\n",
    "# Step 6: Example usage of the function with a sample input\n",
    "example_text = \"Michael Smith (msmith@gmail.com, (+1) 111-111-1111) committed a mistake when he used PyTorch Trainer instead of HF Trainer.\"\n",
    "\n",
    "# Send the request and print the response\n",
    "summary, detected_pii = send_huggingface_request(\n",
    "    text=f\"{example_text}\\n\\nSummarize the above text in 1-2 sentences.\"\n",
    ")\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Detected PII:\", detected_pii)\n",
    "\n",
    "# Step 7: Testing PII Scrubbing on Real Data (Optional)\n",
    "# You can load a dataset containing PII and apply the scrubbing mechanism to it\n",
    "\n",
    "# Load the AI4Privacy PII Masking dataset\n",
    "pii_ds = load_dataset(\"ai4privacy/pii-masking-200k\")\n",
    "\n",
    "# Example input from the dataset\n",
    "example_text = pii_ds[\"train\"][36][\"source_text\"]\n",
    "\n",
    "# Send the request with the scrubbed dataset example\n",
    "summary, detected_pii = send_huggingface_request(\n",
    "    text=f\"{example_text}\\n\\nSummarize the above text in 1-2 sentences.\"\n",
    ")\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Detected PII:\", detected_pii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T06:30:44.231863Z",
     "iopub.status.busy": "2024-12-26T06:30:44.231405Z",
     "iopub.status.idle": "2024-12-26T06:30:48.564897Z",
     "shell.execute_reply": "2024-12-26T06:30:48.563800Z",
     "shell.execute_reply.started": "2024-12-26T06:30:44.231830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available recognizers in Presidio: ['UsBankRecognizer', 'SpacyRecognizer', 'AuAbnRecognizer', 'IbanRecognizer', 'InAadhaarRecognizer', 'InPanRecognizer', 'AuMedicareRecognizer', 'InPassportRecognizer', 'MedicalLicenseRecognizer', 'PhoneRecognizer', 'InVehicleRegistrationRecognizer', 'CryptoRecognizer', 'IpRecognizer', 'UsItinRecognizer', 'SgFinRecognizer', 'UsLicenseRecognizer', 'UrlRecognizer', 'AuTfnRecognizer', 'DateRecognizer', 'UsSsnRecognizer', 'EmailRecognizer', 'AuAcnRecognizer', 'UsPassportRecognizer', 'NhsRecognizer', 'InVoterRecognizer', 'UkNinoRecognizer', 'CreditCardRecognizer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized text using Presidio: text: <PERSON> email is <EMAIL_ADDRESS> and his phone number is +<US_BANK_NUMBER>.\n",
      "items:\n",
      "[\n",
      "    {'start': 59, 'end': 75, 'entity_type': 'US_BANK_NUMBER', 'text': '<US_BANK_NUMBER>', 'operator': 'replace'},\n",
      "    {'start': 18, 'end': 33, 'entity_type': 'EMAIL_ADDRESS', 'text': '<EMAIL_ADDRESS>', 'operator': 'replace'},\n",
      "    {'start': 0, 'end': 8, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected PII using Hugging Face: []\n",
      "Detected PII using Hugging Face: []\n",
      "Comparing Presidio and Hugging Face NER Results:\n",
      "Presidio detected PII: [type: EMAIL_ADDRESS, start: 20, end: 40, score: 1.0, type: PERSON, start: 0, end: 10, score: 0.85, type: URL, start: 20, end: 27, score: 0.5, type: URL, start: 29, end: 40, score: 0.5, type: US_BANK_NUMBER, start: 66, end: 76, score: 0.05, type: US_DRIVER_LICENSE, start: 66, end: 76, score: 0.01]\n",
      "Hugging Face detected PII: []\n",
      "Customizable Recognizers: [<presidio_analyzer.predefined_recognizers.us_bank_recognizer.UsBankRecognizer object at 0x7fb493b1d810>, <presidio_analyzer.predefined_recognizers.spacy_recognizer.SpacyRecognizer object at 0x7fb493b1d840>, <presidio_analyzer.predefined_recognizers.au_abn_recognizer.AuAbnRecognizer object at 0x7fb493b1d870>, <presidio_analyzer.predefined_recognizers.iban_recognizer.IbanRecognizer object at 0x7fb493b1d8a0>, <presidio_analyzer.predefined_recognizers.in_aadhaar_recognizer.InAadhaarRecognizer object at 0x7fb493b1d900>, <presidio_analyzer.predefined_recognizers.in_pan_recognizer.InPanRecognizer object at 0x7fb493b1d930>, <presidio_analyzer.predefined_recognizers.au_medicare_recognizer.AuMedicareRecognizer object at 0x7fb493b1d960>, <presidio_analyzer.predefined_recognizers.in_passport_recognizer.InPassportRecognizer object at 0x7fb493b1d9c0>, <presidio_analyzer.predefined_recognizers.medical_license_recognizer.MedicalLicenseRecognizer object at 0x7fb493b1d9f0>, <presidio_analyzer.predefined_recognizers.phone_recognizer.PhoneRecognizer object at 0x7fb493b1fa00>, <presidio_analyzer.predefined_recognizers.in_vehicle_registration_recognizer.InVehicleRegistrationRecognizer object at 0x7fb493b1da20>, <presidio_analyzer.predefined_recognizers.crypto_recognizer.CryptoRecognizer object at 0x7fb493b1da50>, <presidio_analyzer.predefined_recognizers.ip_recognizer.IpRecognizer object at 0x7fb493b1dab0>, <presidio_analyzer.predefined_recognizers.us_itin_recognizer.UsItinRecognizer object at 0x7fb493b1d300>, <presidio_analyzer.predefined_recognizers.sg_fin_recognizer.SgFinRecognizer object at 0x7fb493b1d330>, <presidio_analyzer.predefined_recognizers.us_driver_license_recognizer.UsLicenseRecognizer object at 0x7fb493b1d360>, <presidio_analyzer.predefined_recognizers.url_recognizer.UrlRecognizer object at 0x7fb493b1dba0>, <presidio_analyzer.predefined_recognizers.au_tfn_recognizer.AuTfnRecognizer object at 0x7fb493b1d3c0>, <presidio_analyzer.predefined_recognizers.date_recognizer.DateRecognizer object at 0x7fb493b1dbd0>, <presidio_analyzer.predefined_recognizers.us_ssn_recognizer.UsSsnRecognizer object at 0x7fb493b1d3f0>, <presidio_analyzer.predefined_recognizers.email_recognizer.EmailRecognizer object at 0x7fb493b1dc00>, <presidio_analyzer.predefined_recognizers.au_acn_recognizer.AuAcnRecognizer object at 0x7fb493b1d450>, <presidio_analyzer.predefined_recognizers.us_passport_recognizer.UsPassportRecognizer object at 0x7fb493b1d4b0>, <presidio_analyzer.predefined_recognizers.uk_nhs_recognizer.NhsRecognizer object at 0x7fb493b1d570>, <presidio_analyzer.predefined_recognizers.in_voter_recognizer.InVoterRecognizer object at 0x7fb493b1de10>, <presidio_analyzer.predefined_recognizers.uk_nino_recognizer.UkNinoRecognizer object at 0x7fb493b1d720>, <presidio_analyzer.predefined_recognizers.credit_card_recognizer.CreditCardRecognizer object at 0x7fb50212bfa0>]\n",
      "Custom Anonymized text using Presidio: text: <PERSON> was seen at <LOCATION> and her email is <EMAIL_ADDRESS>.\n",
      "items:\n",
      "[\n",
      "    {'start': 49, 'end': 64, 'entity_type': 'EMAIL_ADDRESS', 'text': '<EMAIL_ADDRESS>', 'operator': 'replace'},\n",
      "    {'start': 21, 'end': 31, 'entity_type': 'LOCATION', 'text': '<LOCATION>', 'operator': 'replace'},\n",
      "    {'start': 0, 'end': 8, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "\n",
    "# Presidio is for PII detection and redaction\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine, OperatorConfig\n",
    "from presidio_analyzer import RecognizerResult\n",
    "\n",
    "# Hugging Face's transformers for NER (Named Entity Recognition)\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. Setup Presidio Analyzer\n",
    "\n",
    "# Initialize Presidio's AnalyzerEngine, which will detect various PII categories\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# List available PII recognizers in Presidio\n",
    "available_recognizers = analyzer.get_recognizers()\n",
    "print(\"Available recognizers in Presidio:\", [r.name for r in available_recognizers])\n",
    "\n",
    "# 3. Setup Hugging Face for NER\n",
    "\n",
    "# Using Hugging Face pipeline for Named Entity Recognition (NER)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# 4. Function to detect and redact PII using Presidio\n",
    "\n",
    "def detect_and_redact_pii_with_presidio(text: str):\n",
    "    \"\"\"\n",
    "    This function uses Presidio to detect and redact PII from the text.\n",
    "    \"\"\"\n",
    "    # Analyze the text to detect PII\n",
    "    results = analyzer.analyze(text=text, language='en')\n",
    "    \n",
    "    # Create an anonymizer engine\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    \n",
    "    # Redact detected PII\n",
    "    anonymized_text = anonymizer.anonymize(text, results)\n",
    "    \n",
    "    # Return the anonymized text and detected entities\n",
    "    return anonymized_text, results\n",
    "\n",
    "# Example input text with PII\n",
    "input_text = \"John Doe's email is john.doe@example.com and his phone number is +1234567890.\"\n",
    "\n",
    "# Detect and redact PII using Presidio\n",
    "anonymized_text_presidio, detected_pii_presidio = detect_and_redact_pii_with_presidio(input_text)\n",
    "print(\"Anonymized text using Presidio:\", anonymized_text_presidio)\n",
    "\n",
    "# 5. Function to detect PII using Hugging Face's NER pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Using Hugging Face's NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Function to detect PII using Hugging Face's NER pipeline\n",
    "def detect_pii_with_huggingface(text: str):\n",
    "    \"\"\"\n",
    "    This function uses Hugging Face's NER model to detect PII in the text.\n",
    "    \"\"\"\n",
    "    # Detect named entities using Hugging Face's NER pipeline\n",
    "    ner_results = ner_pipeline(text)\n",
    "    \n",
    "    # Filter out non-PII entities (e.g., labels other than 'PER' for persons, 'ORG' for organizations)\n",
    "    pii_entities = [entity for entity in ner_results if entity['entity'] in ['PER', 'LOC', 'ORG', 'MISC']]\n",
    "    \n",
    "    return pii_entities\n",
    "\n",
    "# Example input text with PII\n",
    "input_text = \"John Doe's email is john.doe@example.com and his phone number is +1234567890.\"\n",
    "\n",
    "# Detect PII using Hugging Face\n",
    "detected_pii_huggingface = detect_pii_with_huggingface(input_text)\n",
    "print(\"Detected PII using Hugging Face:\", detected_pii_huggingface)\n",
    "\n",
    "\n",
    "# Detect PII using Hugging Face\n",
    "detected_pii_huggingface = detect_pii_with_huggingface(input_text)\n",
    "print(\"Detected PII using Hugging Face:\", detected_pii_huggingface)\n",
    "\n",
    "# 6. Compare the results from both methods\n",
    "\n",
    "print(\"Comparing Presidio and Hugging Face NER Results:\")\n",
    "print(f\"Presidio detected PII: {detected_pii_presidio}\")\n",
    "print(f\"Hugging Face detected PII: {detected_pii_huggingface}\")\n",
    "\n",
    "# 7. Customizing Presidio for specific PII types\n",
    "\n",
    "# Example: Adding custom recognizers or modifying existing ones\n",
    "custom_recognizers = analyzer.get_recognizers()\n",
    "print(\"Customizable Recognizers:\", custom_recognizers)\n",
    "\n",
    "# Presidio allows customization of the recognizers, for example, to redact specific keywords\n",
    "def custom_pii_redaction(text: str):\n",
    "    \"\"\"\n",
    "    Custom PII redaction using custom recognizers in Presidio\n",
    "    \"\"\"\n",
    "    custom_results = analyzer.analyze(text=text, language='en')\n",
    "    custom_anonymizer = AnonymizerEngine()\n",
    "    custom_anonymized_text = custom_anonymizer.anonymize(text, custom_results)\n",
    "    return custom_anonymized_text\n",
    "\n",
    "# Example input with custom text\n",
    "custom_input_text = \"Jane Doe was seen at 1234 Elm Street and her email is jane.doe@customdomain.com.\"\n",
    "custom_anonymized_text = custom_pii_redaction(custom_input_text)\n",
    "print(\"Custom Anonymized text using Presidio:\", custom_anonymized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
