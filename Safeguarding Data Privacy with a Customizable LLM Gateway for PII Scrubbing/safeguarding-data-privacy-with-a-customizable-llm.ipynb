{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, we explore how to manage **Personally Identifiable Information (PII)** in text data using two different approaches:\n",
    "\n",
    "1. **Presidio-based PII Redaction**: This part demonstrates how to use the **Presidio** library, an open-source tool for detecting and anonymizing PII, such as names, email addresses, and phone numbers. The notebook explains how to set up Presidio's Analyzer and Anonymizer, providing a solution for identifying and redacting sensitive information from text.\n",
    "\n",
    "2. **Hugging Face's Named Entity Recognition (NER) with Custom Scrubbers**: In this section, we use Hugging Face's **NER model** (BERT-based) for detecting PII in text, and custom **scrubber functions** to redact various types of PII, such as phone numbers, credit card details, and email addresses. This part also integrates a **summarization** function that generates a summary of the scrubbed text, demonstrating how to preprocess and protect sensitive data while maintaining the utility of the text.\n",
    "\n",
    "Together, these approaches provide two ways to handle PII in text data, ensuring privacy and data security while still allowing for text processing, summarization, and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall presidio-analyzer presidio-anonymizer\n",
    "# !pip install presidio-analyzer presidio-anonymizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title: PII Detection and Scrubbing with Hugging Face's NER and Custom Scrubbers\n",
    "\n",
    "This code demonstrates how to detect and scrub **Personally Identifiable Information (PII)** from text using **Hugging Face's NER** (Named Entity Recognition) models and custom scrubbing functions. Here's a breakdown of the steps:\n",
    "\n",
    "1. **Import Necessary Libraries**\n",
    "   - **Transformers**: For using Hugging Face's pre-trained NER model.\n",
    "   - **Datasets**: For loading the PII Masking dataset.\n",
    "   - **Torch**: Required for model execution.\n",
    "\n",
    "2. **Load Hugging Face's NER Pipeline**\n",
    "   - Initializes a **Hugging Face NER pipeline** with a pre-trained **BERT model** fine-tuned on the CoNLL-03 dataset for NER tasks.\n",
    "\n",
    "3. **Define PII Scrubbers**\n",
    "   - Defines functions to scrub various types of PII:\n",
    "     - **Phone Numbers**: Scrubs phone numbers from the text.\n",
    "     - **Credit Card Numbers**: Scrubs credit card numbers.\n",
    "     - **Email Addresses**: Scrubs email addresses.\n",
    "     - **Postal Codes**: Scrubs postal codes.\n",
    "     - **SIN Numbers**: Scrubs Social Insurance Numbers (SIN).\n",
    "   - Each function uses **regular expressions (regex)** to detect and replace PII with placeholder text like `[REDACTED PHONE NUMBER]`.\n",
    "\n",
    "4. **Apply Scrubbers to Input Text**\n",
    "   - Combines all scrubbers into a list and applies them to the input text via the `apply_scrubbers` function.\n",
    "\n",
    "5. **PII Detection with Hugging Face**\n",
    "   - The `detect_pii_with_huggingface` function uses the Hugging Face NER pipeline to detect PII in the text, specifically filtering entities such as **persons (PER)**, **locations (LOC)**, **organizations (ORG)**, and **miscellaneous entities (MISC)**.\n",
    "\n",
    "6. **Hugging Face Request with Scrubbed Text**\n",
    "   - The `send_huggingface_request` function first detects and scrubs PII from the input text, then sends a request to a Hugging Face model for text summarization (using **BART** in this case).\n",
    "   - It returns the **summary** of the scrubbed text and the **detected PII**.\n",
    "\n",
    "7. **Example Usage**\n",
    "   - Example input text: `\"Michael Smith (msmith@gmail.com, (+1) 111-111-1111) committed a mistake when he used PyTorch Trainer instead of HF Trainer.\"`\n",
    "   - The code summarizes the input text after scrubbing the PII and detects PII entities like **name**, **email**, and **phone number**.\n",
    "\n",
    "8. **Testing PII Scrubbing on Real Data (Optional)**\n",
    "   - Loads the **AI4Privacy PII Masking dataset** containing text with PII.\n",
    "   - Applies the scrubbing mechanism to an example text from the dataset and prints the detected PII and the summary of the scrubbed text.\n",
    "\n",
    "### Key Steps:\n",
    "- **PII Detection**: Detect PII using **Hugging Face's NER model**.\n",
    "- **PII Scrubbing**: Scrub detected PII using custom scrubbers with **regex**.\n",
    "- **Text Summarization**: Generate a summary of the scrubbed text using Hugging Face's **BART** model.\n",
    "- **PII Masking on Real Data**: Optionally test PII scrubbing on a dataset with real-world examples.\n",
    "\n",
    "This workflow helps with **privacy protection** by detecting and scrubbing PII from texts before processing, generating insights without compromising personal information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T06:33:05.978233Z",
     "iopub.status.busy": "2024-12-26T06:33:05.977760Z",
     "iopub.status.idle": "2024-12-26T06:33:52.580005Z",
     "shell.execute_reply": "2024-12-26T06:33:52.578676Z",
     "shell.execute_reply.started": "2024-12-26T06:33:05.978200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Your max_length is set to 50, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [{'summary_text': 'Michael Smith committed a mistake when he used PyTorch Trainer instead of HF Trainer. He used the wrong name for the training program.'}]\n",
      "Detected PII: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [{'summary_text': \"I need the latest update on assessment results. Please send the files to [REDACTED EMAIL ADDRESS]. For your extra time, we'll offer you Kip[REDACTED PHONE NUMBER]. But please provide your л\"}]\n",
      "Detected PII: []\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Set your environment variables (API keys, if needed, for cloud models)\n",
    "# For Hugging Face models, you generally don’t need an API key unless using specific models on Hugging Face Hub\n",
    "\n",
    "# Step 1: Load Hugging Face's NER pipeline\n",
    "# We will use a pretrained BERT model for Named Entity Recognition (NER)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Step 2: Define scrubbers for different types of PII\n",
    "def scrub_phone_numbers(text: str) -> str:\n",
    "    return re.sub(r\"\\(?\\+?[0-9]*\\)?[-.\\s]?[0-9]+[-.\\s]?[0-9]+[-.\\s]?[0-9]+\", \"[REDACTED PHONE NUMBER]\", text)\n",
    "\n",
    "def scrub_credit_card_numbers(text: str) -> str:\n",
    "    return re.sub(r\"\\b(?:\\d[ -]*?){13,16}\\b\", \"[REDACTED CREDIT CARD]\", text)\n",
    "\n",
    "def scrub_email_addresses(text: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"[REDACTED EMAIL ADDRESS]\", text)\n",
    "\n",
    "def scrub_postal_codes(text: str) -> str:\n",
    "    return re.sub(r\"\\b\\d{5}(-\\d{4})?\\b\", \"[REDACTED POSTAL CODE]\", text)\n",
    "\n",
    "def scrub_sin_numbers(text: str) -> str:\n",
    "    return re.sub(r\"\\b\\d{3}-\\d{3}-\\d{3}\\b\", \"[REDACTED SIN]\", text)\n",
    "\n",
    "# Combine all scrubbers into a list\n",
    "ALL_SCRUBBERS = [\n",
    "    scrub_phone_numbers,\n",
    "    scrub_credit_card_numbers,\n",
    "    scrub_email_addresses,\n",
    "    scrub_postal_codes,\n",
    "    scrub_sin_numbers,\n",
    "]\n",
    "\n",
    "# Step 3: Function to apply all scrubbers to input text\n",
    "def apply_scrubbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply all predefined scrubbers to the input text.\n",
    "    \"\"\"\n",
    "    for scrubber in ALL_SCRUBBERS:\n",
    "        text = scrubber(text)\n",
    "    return text\n",
    "\n",
    "# Step 4: Function to detect PII using Hugging Face's NER pipeline\n",
    "def detect_pii_with_huggingface(text: str):\n",
    "    \"\"\"\n",
    "    This function uses Hugging Face's NER model to detect PII in the text.\n",
    "    \"\"\"\n",
    "    # Detect named entities using Hugging Face's NER pipeline\n",
    "    ner_results = ner_pipeline(text)\n",
    "    \n",
    "    # Filter out non-PII entities (e.g., labels other than 'PER' for persons, 'ORG' for organizations)\n",
    "    pii_entities = [entity for entity in ner_results if entity['entity'] in ['PER', 'LOC', 'ORG', 'MISC']]\n",
    "    \n",
    "    return pii_entities\n",
    "\n",
    "# Step 5: Set up a function to send a request to a Hugging Face model (e.g., summarization or generation)\n",
    "def send_huggingface_request(text: str, model=\"facebook/bart-large-cnn\", max_tokens=50):\n",
    "    \"\"\"\n",
    "    Sends a request to a Hugging Face model after scrubbing PII.\n",
    "    \"\"\"\n",
    "    # Step 5.1: Detect and scrub PII using Hugging Face's NER model\n",
    "    detected_pii = detect_pii_with_huggingface(text)\n",
    "    scrubbed_text = apply_scrubbers(text)\n",
    "    \n",
    "    # Step 5.2: Load the Hugging Face model (for text summarization or generation)\n",
    "    summarizer = pipeline(\"summarization\", model=model)\n",
    "    \n",
    "    # Step 5.3: Generate a summary of the scrubbed text\n",
    "    summary = summarizer(scrubbed_text, max_length=max_tokens, min_length=25, do_sample=False)\n",
    "    \n",
    "    return summary, detected_pii\n",
    "\n",
    "# Step 6: Example usage of the function with a sample input\n",
    "example_text = \"Michael Smith (msmith@gmail.com, (+1) 111-111-1111) committed a mistake when he used PyTorch Trainer instead of HF Trainer.\"\n",
    "\n",
    "# Send the request and print the response\n",
    "summary, detected_pii = send_huggingface_request(\n",
    "    text=f\"{example_text}\\n\\nSummarize the above text in 1-2 sentences.\"\n",
    ")\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Detected PII:\", detected_pii)\n",
    "\n",
    "# Step 7: Testing PII Scrubbing on Real Data (Optional)\n",
    "# You can load a dataset containing PII and apply the scrubbing mechanism to it\n",
    "\n",
    "# Load the AI4Privacy PII Masking dataset\n",
    "pii_ds = load_dataset(\"ai4privacy/pii-masking-200k\")\n",
    "\n",
    "# Example input from the dataset\n",
    "example_text = pii_ds[\"train\"][36][\"source_text\"]\n",
    "\n",
    "# Send the request with the scrubbed dataset example\n",
    "summary, detected_pii = send_huggingface_request(\n",
    "    text=f\"{example_text}\\n\\nSummarize the above text in 1-2 sentences.\"\n",
    ")\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Detected PII:\", detected_pii)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title: PII Detection and Redaction using Presidio and Hugging Face NER Models\n",
    "\n",
    "This code demonstrates how to use **Presidio** and **Hugging Face's NER models** for **PII detection and redaction**. Below is a breakdown of the steps performed:\n",
    "\n",
    "1. **Import Necessary Libraries**\n",
    "   - **Presidio**: For detecting and redacting PII (using `AnalyzerEngine` and `AnonymizerEngine`).\n",
    "   - **Hugging Face**: For Named Entity Recognition (NER) using a pre-trained BERT model.\n",
    "\n",
    "2. **Setup Presidio Analyzer**\n",
    "   - Initializes **Presidio's AnalyzerEngine**, which is responsible for detecting various PII categories (e.g., names, emails, phone numbers).\n",
    "   - Prints the available PII recognizers in Presidio.\n",
    "\n",
    "3. **Setup Hugging Face NER Pipeline**\n",
    "   - Loads a **Hugging Face NER pipeline** using a pre-trained **BERT model** fine-tuned for NER tasks. This model identifies entities like **persons**, **locations**, **organizations**, and **miscellaneous entities** in text.\n",
    "\n",
    "4. **PII Detection and Redaction with Presidio**\n",
    "   - The function `detect_and_redact_pii_with_presidio(text)` uses **Presidio** to detect and redact PII. \n",
    "   - It analyzes the text with the `AnalyzerEngine`, and then it anonymizes the detected PII using the `AnonymizerEngine`.\n",
    "   - Returns the **anonymized text** and the **detected PII**.\n",
    "\n",
    "5. **PII Detection with Hugging Face**\n",
    "   - The function `detect_pii_with_huggingface(text)` uses **Hugging Face's NER pipeline** to detect PII entities in the text.\n",
    "   - It filters and returns entities like **persons (PER)**, **locations (LOC)**, **organizations (ORG)**, and **miscellaneous (MISC)**.\n",
    "\n",
    "6. **Comparison of Results**\n",
    "   - The code compares the results from **Presidio** and **Hugging Face** for detecting PII in the same input text and prints out both results.\n",
    "\n",
    "7. **Customizing Presidio for Specific PII Types**\n",
    "   - Shows how **Presidio** can be customized by adding or modifying PII recognizers to detect specific keywords or types of information.\n",
    "   - Demonstrates **custom redaction** using **custom recognizers** to anonymize custom data (e.g., redacting certain keywords).\n",
    "\n",
    "### Example Input:\n",
    "The text used for testing includes **John Doe's email** and **phone number**: \n",
    "\n",
    "```text\n",
    "\"John Doe's email is john.doe@example.com and his phone number is +1234567890.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Available recognizers in Presidio: ['InVoterRecognizer', 'SpacyRecognizer', 'UsLicenseRecognizer', 'UsBankRecognizer', 'InPassportRecognizer', 'UsSsnRecognizer', 'DateRecognizer', 'CryptoRecognizer', 'AuTfnRecognizer', 'UsItinRecognizer', 'UsPassportRecognizer', 'NhsRecognizer', 'UkNinoRecognizer', 'AuMedicareRecognizer', 'InVehicleRegistrationRecognizer', 'AuAbnRecognizer', 'AuAcnRecognizer', 'InPanRecognizer', 'InAadhaarRecognizer', 'PhoneRecognizer', 'IbanRecognizer', 'EmailRecognizer', 'SgFinRecognizer', 'UrlRecognizer', 'CreditCardRecognizer', 'IpRecognizer', 'MedicalLicenseRecognizer']\n",
      "WARNING:tensorflow:From c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized text using Presidio: text: <PERSON> email is <EMAIL_ADDRESS> and his phone number is +<US_BANK_NUMBER>.\n",
      "items:\n",
      "[\n",
      "    {'start': 59, 'end': 75, 'entity_type': 'US_BANK_NUMBER', 'text': '<US_BANK_NUMBER>', 'operator': 'replace'},\n",
      "    {'start': 18, 'end': 33, 'entity_type': 'EMAIL_ADDRESS', 'text': '<EMAIL_ADDRESS>', 'operator': 'replace'},\n",
      "    {'start': 0, 'end': 8, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected PII using Hugging Face: []\n",
      "Detected PII using Hugging Face: []\n",
      "Comparing Presidio and Hugging Face NER Results:\n",
      "Presidio detected PII: [type: EMAIL_ADDRESS, start: 20, end: 40, score: 1.0, type: PERSON, start: 0, end: 10, score: 0.85, type: URL, start: 20, end: 27, score: 0.5, type: URL, start: 29, end: 40, score: 0.5, type: US_BANK_NUMBER, start: 66, end: 76, score: 0.05, type: US_DRIVER_LICENSE, start: 66, end: 76, score: 0.01]\n",
      "Hugging Face detected PII: []\n",
      "Customizable Recognizers: [<presidio_analyzer.predefined_recognizers.in_voter_recognizer.InVoterRecognizer object at 0x00000193D3898820>, <presidio_analyzer.predefined_recognizers.spacy_recognizer.SpacyRecognizer object at 0x00000193D3898040>, <presidio_analyzer.predefined_recognizers.us_driver_license_recognizer.UsLicenseRecognizer object at 0x00000193D38980A0>, <presidio_analyzer.predefined_recognizers.us_bank_recognizer.UsBankRecognizer object at 0x00000193D38980D0>, <presidio_analyzer.predefined_recognizers.in_passport_recognizer.InPassportRecognizer object at 0x00000193D3898100>, <presidio_analyzer.predefined_recognizers.us_ssn_recognizer.UsSsnRecognizer object at 0x00000193D3898160>, <presidio_analyzer.predefined_recognizers.date_recognizer.DateRecognizer object at 0x00000193D3898190>, <presidio_analyzer.predefined_recognizers.crypto_recognizer.CryptoRecognizer object at 0x00000193D38981C0>, <presidio_analyzer.predefined_recognizers.au_tfn_recognizer.AuTfnRecognizer object at 0x00000193D38981F0>, <presidio_analyzer.predefined_recognizers.us_itin_recognizer.UsItinRecognizer object at 0x00000193D3898220>, <presidio_analyzer.predefined_recognizers.us_passport_recognizer.UsPassportRecognizer object at 0x00000193D3898250>, <presidio_analyzer.predefined_recognizers.uk_nhs_recognizer.NhsRecognizer object at 0x00000193D3898280>, <presidio_analyzer.predefined_recognizers.uk_nino_recognizer.UkNinoRecognizer object at 0x00000193D38982B0>, <presidio_analyzer.predefined_recognizers.au_medicare_recognizer.AuMedicareRecognizer object at 0x00000193D3898310>, <presidio_analyzer.predefined_recognizers.in_vehicle_registration_recognizer.InVehicleRegistrationRecognizer object at 0x00000193D3898370>, <presidio_analyzer.predefined_recognizers.au_abn_recognizer.AuAbnRecognizer object at 0x00000193D38983D0>, <presidio_analyzer.predefined_recognizers.au_acn_recognizer.AuAcnRecognizer object at 0x00000193D3898400>, <presidio_analyzer.predefined_recognizers.in_pan_recognizer.InPanRecognizer object at 0x00000193D3898490>, <presidio_analyzer.predefined_recognizers.in_aadhaar_recognizer.InAadhaarRecognizer object at 0x00000193D38984C0>, <presidio_analyzer.predefined_recognizers.phone_recognizer.PhoneRecognizer object at 0x00000193D3898580>, <presidio_analyzer.predefined_recognizers.iban_recognizer.IbanRecognizer object at 0x00000193D3898640>, <presidio_analyzer.predefined_recognizers.email_recognizer.EmailRecognizer object at 0x00000193D3898670>, <presidio_analyzer.predefined_recognizers.sg_fin_recognizer.SgFinRecognizer object at 0x00000193D38986A0>, <presidio_analyzer.predefined_recognizers.url_recognizer.UrlRecognizer object at 0x00000193D38986D0>, <presidio_analyzer.predefined_recognizers.credit_card_recognizer.CreditCardRecognizer object at 0x00000193F97A6710>, <presidio_analyzer.predefined_recognizers.ip_recognizer.IpRecognizer object at 0x00000193D3898760>, <presidio_analyzer.predefined_recognizers.medical_license_recognizer.MedicalLicenseRecognizer object at 0x00000193D3898790>]\n",
      "Custom Anonymized text using Presidio: text: <PERSON> was seen at <LOCATION> and her email is <EMAIL_ADDRESS>.\n",
      "items:\n",
      "[\n",
      "    {'start': 49, 'end': 64, 'entity_type': 'EMAIL_ADDRESS', 'text': '<EMAIL_ADDRESS>', 'operator': 'replace'},\n",
      "    {'start': 21, 'end': 31, 'entity_type': 'LOCATION', 'text': '<LOCATION>', 'operator': 'replace'},\n",
      "    {'start': 0, 'end': 8, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "\n",
    "# Presidio is for PII detection and redaction\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine, OperatorConfig\n",
    "from presidio_analyzer import RecognizerResult\n",
    "\n",
    "# Hugging Face's transformers for NER (Named Entity Recognition)\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. Setup Presidio Analyzer\n",
    "\n",
    "# Initialize Presidio's AnalyzerEngine, which will detect various PII categories\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# List available PII recognizers in Presidio\n",
    "available_recognizers = analyzer.get_recognizers()\n",
    "print(\"Available recognizers in Presidio:\", [r.name for r in available_recognizers])\n",
    "\n",
    "# 3. Setup Hugging Face for NER\n",
    "\n",
    "# Using Hugging Face pipeline for Named Entity Recognition (NER)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# 4. Function to detect and redact PII using Presidio\n",
    "\n",
    "def detect_and_redact_pii_with_presidio(text: str):\n",
    "    \"\"\"\n",
    "    This function uses Presidio to detect and redact PII from the text.\n",
    "    \"\"\"\n",
    "    # Analyze the text to detect PII\n",
    "    results = analyzer.analyze(text=text, language='en')\n",
    "    \n",
    "    # Create an anonymizer engine\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    \n",
    "    # Redact detected PII\n",
    "    anonymized_text = anonymizer.anonymize(text, results)\n",
    "    \n",
    "    # Return the anonymized text and detected entities\n",
    "    return anonymized_text, results\n",
    "\n",
    "# Example input text with PII\n",
    "input_text = \"John Doe's email is john.doe@example.com and his phone number is +1234567890.\"\n",
    "\n",
    "# Detect and redact PII using Presidio\n",
    "anonymized_text_presidio, detected_pii_presidio = detect_and_redact_pii_with_presidio(input_text)\n",
    "print(\"Anonymized text using Presidio:\", anonymized_text_presidio)\n",
    "\n",
    "# 5. Function to detect PII using Hugging Face's NER pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Using Hugging Face's NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Function to detect PII using Hugging Face's NER pipeline\n",
    "def detect_pii_with_huggingface(text: str):\n",
    "    \"\"\"\n",
    "    This function uses Hugging Face's NER model to detect PII in the text.\n",
    "    \"\"\"\n",
    "    # Detect named entities using Hugging Face's NER pipeline\n",
    "    ner_results = ner_pipeline(text)\n",
    "    \n",
    "    # Filter out non-PII entities (e.g., labels other than 'PER' for persons, 'ORG' for organizations)\n",
    "    pii_entities = [entity for entity in ner_results if entity['entity'] in ['PER', 'LOC', 'ORG', 'MISC']]\n",
    "    \n",
    "    return pii_entities\n",
    "\n",
    "# Example input text with PII\n",
    "input_text = \"John Doe's email is john.doe@example.com and his phone number is +1234567890.\"\n",
    "\n",
    "# Detect PII using Hugging Face\n",
    "detected_pii_huggingface = detect_pii_with_huggingface(input_text)\n",
    "print(\"Detected PII using Hugging Face:\", detected_pii_huggingface)\n",
    "\n",
    "\n",
    "# Detect PII using Hugging Face\n",
    "detected_pii_huggingface = detect_pii_with_huggingface(input_text)\n",
    "print(\"Detected PII using Hugging Face:\", detected_pii_huggingface)\n",
    "\n",
    "# 6. Compare the results from both methods\n",
    "\n",
    "print(\"Comparing Presidio and Hugging Face NER Results:\")\n",
    "print(f\"Presidio detected PII: {detected_pii_presidio}\")\n",
    "print(f\"Hugging Face detected PII: {detected_pii_huggingface}\")\n",
    "\n",
    "# 7. Customizing Presidio for specific PII types\n",
    "\n",
    "# Example: Adding custom recognizers or modifying existing ones\n",
    "custom_recognizers = analyzer.get_recognizers()\n",
    "print(\"Customizable Recognizers:\", custom_recognizers)\n",
    "\n",
    "# Presidio allows customization of the recognizers, for example, to redact specific keywords\n",
    "def custom_pii_redaction(text: str):\n",
    "    \"\"\"\n",
    "    Custom PII redaction using custom recognizers in Presidio\n",
    "    \"\"\"\n",
    "    custom_results = analyzer.analyze(text=text, language='en')\n",
    "    custom_anonymizer = AnonymizerEngine()\n",
    "    custom_anonymized_text = custom_anonymizer.anonymize(text, custom_results)\n",
    "    return custom_anonymized_text\n",
    "\n",
    "# Example input with custom text\n",
    "custom_input_text = \"Jane Doe was seen at 1234 Elm Street and her email is jane.doe@customdomain.com.\"\n",
    "custom_anonymized_text = custom_pii_redaction(custom_input_text)\n",
    "print(\"Custom Anonymized text using Presidio:\", custom_anonymized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we demonstrated two techniques for identifying and redacting **PII** from text:\n",
    "\n",
    "- **Presidio** provides a comprehensive, built-in solution for detecting and anonymizing various types of PII.\n",
    "- **Hugging Face's NER** model, combined with custom scrubbers, enables us to detect and redact sensitive information, followed by further text processing such as summarization.\n",
    "\n",
    "These methods can be adapted to suit different requirements for **privacy protection** in real-world applications. The workflow ensures that sensitive data is kept private and secure, while still allowing us to extract meaningful insights from the text. Whether you're working with personal data in a dataset or processing textual content with potential PII, these tools provide flexible and effective ways to manage privacy."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
