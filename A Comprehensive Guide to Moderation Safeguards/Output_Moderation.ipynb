{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook provides a comprehensive approach to **Output Moderation** for text generated by language models or other AI systems. As AI applications increasingly interact with users, ensuring the quality, relevance, and safety of the generated outputs is paramount. \n",
    "\n",
    "The provided code implements key moderation techniques to address critical concerns such as:\n",
    "\n",
    "1. **Hallucination Prevention**: Ensures that generated outputs align with the given context and avoid fabricated or misleading content.\n",
    "2. **Sanitization**: Treats all outputs as potentially harmful, escaping any special characters to mitigate injection attacks.\n",
    "3. **Toxicity Filtering**: Uses a predefined set of toxic words and regular expressions to detect and prevent harmful language.\n",
    "4. **Anomaly Detection**: Identifies outputs with abnormal patterns, such as excessive repetition or nonsensical content.\n",
    "5. **Monitoring and Logging**: Tracks outputs along with contextual metadata to enable thorough analysis and debugging.\n",
    "6. **Feedback Mechanism**: Provides a mechanism for user feedback to refine and improve moderation strategies over time.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this notebook is to act as a **baseline framework** for implementing and enhancing moderation pipelines in AI systems. This solution can be used as a starting point for developers aiming to ensure that their models generate outputs that are:\n",
    "- **Safe** (free from toxic or harmful content),\n",
    "- **Relevant** (contextually appropriate), and\n",
    "- **Trustworthy** (minimally prone to hallucinations or anomalies).\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "This framework is particularly suited for:\n",
    "- Chatbots or conversational AI systems.\n",
    "- Generative AI models producing text summaries or insights.\n",
    "- Applications in sensitive domains such as healthcare, education, or customer support, where content moderation is critical.\n",
    "\n",
    "By following the outlined moderation steps, developers can safeguard their applications, enhance user trust, and align with ethical AI practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Output: Artificial intelligence is the simulation of human intelligence in machines.\n",
      "Sanitized Output: Artificial intelligence is the simulation of human intelligence in machines.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: I hate the concept of AI, it's promoting violence.\n",
      "Sanitized Output: I hate the concept of AI, it&#x27;s promoting violence.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: Hello Hello Hello\n",
      "Sanitized Output: Hello Hello Hello\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: <script>alert('malicious');</script>\n",
      "Sanitized Output: &lt;script&gt;alert(&#x27;malicious&#x27;);&lt;/script&gt;\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: Artificial intelligence is a field that <b>abuses</b> resources.\n",
      "Sanitized Output: Artificial intelligence is a field that &lt;b&gt;abuses&lt;/b&gt; resources.\n",
      "Warning: Hallucinated or irrelevant content detected.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration for Moderation\n",
    "TOXIC_WORDS = {\"hate\", \"violence\", \"malicious\", \"abuse\", \"harm\"}\n",
    "ANOMALY_REPETITION_THRESHOLD = 3  # Number of unique words required to avoid flagging as repetitive\n",
    "MAX_OUTPUT_LENGTH = 500  # Maximum allowed length for output\n",
    "\n",
    "# Functions for Output Moderation\n",
    "def hallucination_prevention(output: str, context: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the output aligns with the provided context to prevent hallucination.\n",
    "    \"\"\"\n",
    "    return context.lower() in output.lower()\n",
    "\n",
    "def treat_as_untrusted(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Treat output as untrusted and sanitize it for safe use.\n",
    "    \"\"\"\n",
    "    return html.escape(output)\n",
    "\n",
    "def filter_toxicity(output: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect and filter toxic content in the output using predefined toxic words.\n",
    "    Uses regex to identify toxic patterns.\n",
    "    \"\"\"\n",
    "    for toxic_word in TOXIC_WORDS:\n",
    "        if re.search(rf\"\\b{toxic_word}\\b\", output, re.IGNORECASE):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def anomaly_detection(output: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect anomalies in output, such as excessive repetition, nonsensical content, or length violations.\n",
    "    \"\"\"\n",
    "    words = output.split()\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    # Check for excessive repetition\n",
    "    if len(unique_words) < ANOMALY_REPETITION_THRESHOLD:\n",
    "        return False\n",
    "    \n",
    "    # Check for excessive length\n",
    "    if len(output) > MAX_OUTPUT_LENGTH:\n",
    "        return False\n",
    "    \n",
    "    # Additional checks can be added as needed (e.g., nonsensical patterns)\n",
    "    return True\n",
    "\n",
    "def monitor_and_log(output: str, context: str, status: str):\n",
    "    \"\"\"\n",
    "    Monitor output and log for analysis and debugging, including timestamps and context.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"output_logs.txt\", \"a\") as log_file:\n",
    "        log_file.write(\n",
    "            f\"[{timestamp}] Context: {context}\\n\"\n",
    "            f\"Output: {output}\\n\"\n",
    "            f\"Status: {status}\\n\\n\"\n",
    "        )\n",
    "\n",
    "def feedback_mechanism(output: str, is_acceptable: bool):\n",
    "    \"\"\"\n",
    "    Allow user feedback to refine future moderation decisions.\n",
    "    \"\"\"\n",
    "    feedback_file = \"moderation_feedback.txt\"\n",
    "    with open(feedback_file, \"a\") as file:\n",
    "        feedback = \"Acceptable\" if is_acceptable else \"Unacceptable\"\n",
    "        file.write(f\"Output: {output}\\nFeedback: {feedback}\\n\\n\")\n",
    "\n",
    "# Main Execution with Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example context and outputs\n",
    "    context = \"Explain the concept of artificial intelligence.\"\n",
    "    outputs = [\n",
    "        \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "        \"I hate the concept of AI, it's promoting violence.\",\n",
    "        \"Hello Hello Hello\",\n",
    "        \"<script>alert('malicious');</script>\",\n",
    "        \"Artificial intelligence is a field that <b>abuses</b> resources.\"\n",
    "    ]\n",
    "\n",
    "    for output in outputs:\n",
    "        print(f\"Original Output: {output}\")\n",
    "\n",
    "        # Treat output as untrusted and sanitize it\n",
    "        safe_output = treat_as_untrusted(output)\n",
    "        print(f\"Sanitized Output: {safe_output}\")\n",
    "\n",
    "        # Check for hallucinations\n",
    "        if not hallucination_prevention(safe_output, context):\n",
    "            status = \"Hallucinated or irrelevant content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            monitor_and_log(safe_output, context, status)\n",
    "            continue\n",
    "\n",
    "        # Filter for toxicity\n",
    "        if not filter_toxicity(safe_output):\n",
    "            status = \"Toxic or harmful content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            monitor_and_log(safe_output, context, status)\n",
    "            feedback_mechanism(safe_output, False)\n",
    "            continue\n",
    "\n",
    "        # Detect anomalies\n",
    "        if not anomaly_detection(safe_output):\n",
    "            status = \"Anomaly detected in content (e.g., repetition, length).\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            monitor_and_log(safe_output, context, status)\n",
    "            feedback_mechanism(safe_output, False)\n",
    "            continue\n",
    "\n",
    "        # Log output for monitoring\n",
    "        status = \"Output accepted.\"\n",
    "        monitor_and_log(safe_output, context, status)\n",
    "        print(\"Output logged successfully.\\n\")\n",
    "\n",
    "        # Collect user feedback (mock example for demonstration)\n",
    "        feedback_mechanism(safe_output, is_acceptable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook provides a **foundational framework** for implementing output moderation in AI systems. The techniques demonstrated here—such as hallucination prevention, toxicity filtering, anomaly detection, and output sanitization—highlight the critical steps needed to ensure that AI-generated content is safe, relevant, and aligned with user expectations.\n",
    "\n",
    "It is important to note that this code serves as a **basic example**, emphasizing high-level principles rather than exhaustive implementation. Developers can use this as a starting point and expand upon it to address specific use cases, incorporate advanced moderation techniques, and integrate with production-level systems. \n",
    "\n",
    "Effective moderation is essential for building trust in AI applications, especially in sensitive or high-stakes domains. This notebook is a reminder of the key considerations that should guide the design of robust and ethical AI systems. Further refinements and domain-specific adjustments are encouraged to fully realize the potential of these principles in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
