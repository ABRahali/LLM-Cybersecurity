{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook provides a comprehensive approach to **Output Moderation** for text generated by language models or other AI systems. As AI applications increasingly interact with users, ensuring the quality, relevance, and safety of the generated outputs is paramount. \n",
    "\n",
    "The provided code implements key moderation techniques to address critical concerns such as:\n",
    "\n",
    "1. **Hallucination Prevention**: Ensures that generated outputs align with the given context and avoid fabricated or misleading content.\n",
    "2. **Sanitization**: Treats all outputs as potentially harmful, escaping any special characters to mitigate injection attacks.\n",
    "3. **Toxicity Filtering**: Uses a predefined set of toxic words and regular expressions to detect and prevent harmful language.\n",
    "4. **Anomaly Detection**: Identifies outputs with abnormal patterns, such as excessive repetition or nonsensical content.\n",
    "5. **Monitoring and Logging**: Tracks outputs along with contextual metadata to enable thorough analysis and debugging.\n",
    "6. **Feedback Mechanism**: Provides a mechanism for user feedback to refine and improve moderation strategies over time.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this notebook is to act as a **baseline framework** for implementing and enhancing moderation pipelines in AI systems. This solution can be used as a starting point for developers aiming to ensure that their models generate outputs that are:\n",
    "- **Safe** (free from toxic or harmful content),\n",
    "- **Relevant** (contextually appropriate), and\n",
    "- **Trustworthy** (minimally prone to hallucinations or anomalies).\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "This framework is particularly suited for:\n",
    "- Chatbots or conversational AI systems.\n",
    "- Generative AI models producing text summaries or insights.\n",
    "- Applications in sensitive domains such as healthcare, education, or customer support, where content moderation is critical.\n",
    "\n",
    "By following the outlined moderation steps, developers can safeguard their applications, enhance user trust, and align with ethical AI practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AND ul.intent_name IN ('hello.yes', 'hi.no')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = 'hello.yes, hi.no'\n",
    "\n",
    "if len(intents) > 0:\n",
    "    intents_list = [f\"'{intent.strip()}'\" for intent in intents.split(\",\")]\n",
    "    intent_query = \"AND ul.intent_name IN ({0})\".format(\", \".join(intents_list))\n",
    "intent_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Output: Artificial intelligence is the simulation of human intelligence in machines.\n",
      "Sanitized Output: Artificial intelligence is the simulation of human intelligence in machines.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: I hate the concept of AI, it's promoting violence.\n",
      "Sanitized Output: I hate the concept of AI, it&#x27;s promoting violence.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: Hello Hello Hello\n",
      "Sanitized Output: Hello Hello Hello\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: <script>alert('malicious');</script>\n",
      "Sanitized Output: &lt;script&gt;alert(&#x27;malicious&#x27;);&lt;/script&gt;\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Original Output: Artificial intelligence is a field that <b>abuses</b> resources.\n",
      "Sanitized Output: Artificial intelligence is a field that &lt;b&gt;abuses&lt;/b&gt; resources.\n",
      "Warning: Hallucinated or irrelevant content detected.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration for Moderation\n",
    "TOXIC_WORDS = {\"hate\", \"violence\", \"malicious\", \"abuse\", \"harm\"}\n",
    "ANOMALY_REPETITION_THRESHOLD = 3  # Number of unique words required to avoid flagging as repetitive\n",
    "MAX_OUTPUT_LENGTH = 500  # Maximum allowed length for output\n",
    "\n",
    "# Functions for Output Moderation\n",
    "def hallucination_prevention(output: str, context: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the output aligns with the provided context to prevent hallucination.\n",
    "    \"\"\"\n",
    "    return context.lower() in output.lower()\n",
    "\n",
    "def treat_as_untrusted(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Treat output as untrusted and sanitize it for safe use.\n",
    "    \"\"\"\n",
    "    return html.escape(output)\n",
    "\n",
    "def filter_toxicity(output: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect and filter toxic content in the output using predefined toxic words.\n",
    "    Uses regex to identify toxic patterns.\n",
    "    \"\"\"\n",
    "    for toxic_word in TOXIC_WORDS:\n",
    "        if re.search(rf\"\\b{toxic_word}\\b\", output, re.IGNORECASE):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def anomaly_detection(output: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect anomalies in output, such as excessive repetition, nonsensical content, or length violations.\n",
    "    \"\"\"\n",
    "    words = output.split()\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    # Check for excessive repetition\n",
    "    if len(unique_words) < ANOMALY_REPETITION_THRESHOLD:\n",
    "        return False\n",
    "    \n",
    "    # Check for excessive length\n",
    "    if len(output) > MAX_OUTPUT_LENGTH:\n",
    "        return False\n",
    "    \n",
    "    # Additional checks can be added as needed (e.g., nonsensical patterns)\n",
    "    return True\n",
    "\n",
    "def monitor_and_log(output: str, context: str, status: str):\n",
    "    \"\"\"\n",
    "    Monitor output and log for analysis and debugging, including timestamps and context.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"output_logs.txt\", \"a\") as log_file:\n",
    "        log_file.write(\n",
    "            f\"[{timestamp}] Context: {context}\\n\"\n",
    "            f\"Output: {output}\\n\"\n",
    "            f\"Status: {status}\\n\\n\"\n",
    "        )\n",
    "\n",
    "def feedback_mechanism(output: str, is_acceptable: bool):\n",
    "    \"\"\"\n",
    "    Allow user feedback to refine future moderation decisions.\n",
    "    \"\"\"\n",
    "    feedback_file = \"moderation_feedback.txt\"\n",
    "    with open(feedback_file, \"a\") as file:\n",
    "        feedback = \"Acceptable\" if is_acceptable else \"Unacceptable\"\n",
    "        file.write(f\"Output: {output}\\nFeedback: {feedback}\\n\\n\")\n",
    "\n",
    "# Main Execution with Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example context and outputs\n",
    "    context = \"Explain the concept of artificial intelligence.\"\n",
    "    outputs = [\n",
    "        \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "        \"I hate the concept of AI, it's promoting violence.\",\n",
    "        \"Hello Hello Hello\",\n",
    "        \"<script>alert('malicious');</script>\",\n",
    "        \"Artificial intelligence is a field that <b>abuses</b> resources.\"\n",
    "    ]\n",
    "\n",
    "    for output in outputs:\n",
    "        print(f\"Original Output: {output}\")\n",
    "\n",
    "        # Treat output as untrusted and sanitize it\n",
    "        safe_output = treat_as_untrusted(output)\n",
    "        print(f\"Sanitized Output: {safe_output}\")\n",
    "\n",
    "        # Check for hallucinations\n",
    "        if not hallucination_prevention(safe_output, context):\n",
    "            status = \"Hallucinated or irrelevant content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            monitor_and_log(safe_output, context, status)\n",
    "            continue\n",
    "\n",
    "        # Filter for toxicity\n",
    "        if not filter_toxicity(safe_output):\n",
    "            status = \"Toxic or harmful content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            monitor_and_log(safe_output, context, status)\n",
    "            feedback_mechanism(safe_output, False)\n",
    "            continue\n",
    "\n",
    "        # Detect anomalies\n",
    "        if not anomaly_detection(safe_output):\n",
    "            status = \"Anomaly detected in content (e.g., repetition, length).\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            monitor_and_log(safe_output, context, status)\n",
    "            feedback_mechanism(safe_output, False)\n",
    "            continue\n",
    "\n",
    "        # Log output for monitoring\n",
    "        status = \"Output accepted.\"\n",
    "        monitor_and_log(safe_output, context, status)\n",
    "        print(\"Output logged successfully.\\n\")\n",
    "\n",
    "        # Collect user feedback (mock example for demonstration)\n",
    "        feedback_mechanism(safe_output, is_acceptable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized Output: Artificial intelligence is the simulation of human intelligence in machines.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Sanitized Output: I hate the concept of AI, it&#x27;s promoting violence.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Sanitized Output: Hello Hello Hello\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Sanitized Output: &lt;script&gt;alert(&#x27;malicious&#x27;);&lt;/script&gt;\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Sanitized Output: Artificial intelligence is a field that &lt;b&gt;abuses&lt;/b&gt; resources.\n",
      "Warning: Hallucinated or irrelevant content detected.\n",
      "Sanitized Output: AI can be a tool for both good and evil. However, there are concerns about misuse.\n",
      "Warning: Hallucinated or irrelevant content detected.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class OutputModeration:\n",
    "    def __init__(self, toxic_keywords=None, repetition_threshold=3, max_output_length=500):\n",
    "        self.toxic_keywords = toxic_keywords or {\"hate\", \"violence\", \"malicious\", \"abuse\", \"harm\", \"terror\"}\n",
    "        self.repetition_threshold = repetition_threshold\n",
    "        self.max_output_length = max_output_length\n",
    "        self.previous_outputs = []\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def hallucination_prevention(self, output: str, context: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the output aligns with the provided context to prevent hallucination.\n",
    "        A hallucination occurs when the output is irrelevant or deviates from the context.\n",
    "        \"\"\"\n",
    "        return context.lower() in output.lower()\n",
    "\n",
    "    def treat_as_untrusted(self, output: str) -> str:\n",
    "        \"\"\"\n",
    "        Treat output as untrusted and sanitize it for safe use.\n",
    "        This includes escaping potentially dangerous characters.\n",
    "        \"\"\"\n",
    "        return html.escape(output)\n",
    "\n",
    "    def filter_toxicity(self, output: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect and filter toxic content in the output using a simple keyword-based approach.\n",
    "        \"\"\"\n",
    "        for word in self.toxic_keywords:\n",
    "            if re.search(rf\"\\b{word}\\b\", output, re.IGNORECASE):\n",
    "                return False  # Toxic content detected\n",
    "        return True  # No toxicity detected\n",
    "\n",
    "    def anomaly_detection(self, output: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect anomalies in output using clustering (DBSCAN).\n",
    "        This can detect outliers and flag unusual or unexpected outputs.\n",
    "        \"\"\"\n",
    "        words = output.split()\n",
    "        unique_words = set(words)\n",
    "\n",
    "        # Check for excessive repetition\n",
    "        if len(unique_words) < self.repetition_threshold:\n",
    "            return False  # Repetition detected\n",
    "        \n",
    "        # Check for excessive length\n",
    "        if len(output) > self.max_output_length:\n",
    "            return False  # Output too long\n",
    "        \n",
    "        # Prepare data for clustering (for simplicity, use word counts)\n",
    "        self.previous_outputs.append(output)\n",
    "        word_counts = [len(output.split()) for output in self.previous_outputs]\n",
    "\n",
    "        # Standardize and cluster using DBSCAN\n",
    "        word_counts_scaled = self.scaler.fit_transform([[count] for count in word_counts])\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "        cluster_labels = dbscan.fit_predict(word_counts_scaled)\n",
    "        \n",
    "        # Anomaly if not belonging to a large cluster\n",
    "        if cluster_labels[-1] == -1:  # Outlier\n",
    "            return False  # Anomaly detected\n",
    "\n",
    "        return True  # No anomalies detected\n",
    "\n",
    "    def monitor_and_log(self, output: str, context: str, status: str):\n",
    "        \"\"\"\n",
    "        Monitor output and log for analysis and debugging, including timestamps, context, and status.\n",
    "        Logs are saved in a log file for future reference and analysis.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            with open(\"output_logs.txt\", \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"[{timestamp}] Context: {context}\\n\"\n",
    "                    f\"Output: {output}\\n\"\n",
    "                    f\"Status: {status}\\n\\n\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging output: {e}\")\n",
    "\n",
    "    def feedback_mechanism(self, output: str, is_acceptable: bool):\n",
    "        \"\"\"\n",
    "        Allow user feedback to refine future moderation decisions.\n",
    "        This feedback helps in training and fine-tuning moderation rules over time.\n",
    "        \"\"\"\n",
    "        feedback_file = \"moderation_feedback.txt\"\n",
    "        try:\n",
    "            with open(feedback_file, \"a\") as file:\n",
    "                feedback = \"Acceptable\" if is_acceptable else \"Unacceptable\"\n",
    "                file.write(f\"Output: {output}\\nFeedback: {feedback}\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving feedback: {e}\")\n",
    "\n",
    "\n",
    "class AsyncOutputProcessor:\n",
    "    def __init__(self, moderation_system: OutputModeration):\n",
    "        self.moderation_system = moderation_system\n",
    "\n",
    "    async def process_output_async(self, output: str, context: str):\n",
    "        \"\"\"\n",
    "        Asynchronously process output for moderation.\n",
    "        \"\"\"\n",
    "        # Treat output as untrusted and sanitize it\n",
    "        safe_output = self.moderation_system.treat_as_untrusted(output)\n",
    "        print(f\"Sanitized Output: {safe_output}\")\n",
    "\n",
    "        # Check for hallucinations\n",
    "        if not self.moderation_system.hallucination_prevention(safe_output, context):\n",
    "            status = \"Hallucinated or irrelevant content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "            return\n",
    "\n",
    "        # Filter for toxicity\n",
    "        if not self.moderation_system.filter_toxicity(safe_output):\n",
    "            status = \"Toxic or harmful content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "            self.moderation_system.feedback_mechanism(safe_output, False)\n",
    "            return\n",
    "\n",
    "        # Detect anomalies\n",
    "        if not self.moderation_system.anomaly_detection(safe_output):\n",
    "            status = \"Anomaly detected in content (e.g., repetition, length, nonsensical patterns).\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "            self.moderation_system.feedback_mechanism(safe_output, False)\n",
    "            return\n",
    "\n",
    "        # Log output for monitoring\n",
    "        status = \"Output accepted.\"\n",
    "        self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "        print(\"Output logged successfully.\\n\")\n",
    "\n",
    "        # Collect user feedback (mock example for demonstration)\n",
    "        self.moderation_system.feedback_mechanism(safe_output, is_acceptable=True)\n",
    "\n",
    "\n",
    "# Main Execution with Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the moderation system and processor\n",
    "    moderation_system = OutputModeration()\n",
    "    async_processor = AsyncOutputProcessor(moderation_system)\n",
    "\n",
    "    # Example context and outputs\n",
    "    context = \"Explain the concept of artificial intelligence.\"\n",
    "    outputs = [\n",
    "        \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "        \"I hate the concept of AI, it's promoting violence.\",\n",
    "        \"Hello Hello Hello\",\n",
    "        \"<script>alert('malicious');</script>\",\n",
    "        \"Artificial intelligence is a field that <b>abuses</b> resources.\",\n",
    "        \"AI can be a tool for both good and evil. However, there are concerns about misuse.\"\n",
    "    ]\n",
    "\n",
    "    # Use asynchronous processing for outputs\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [async_processor.process_output_async(output, context) for output in outputs]\n",
    "    loop.run_until_complete(asyncio.gather(*tasks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "from nemo.collections.nlp.models import TextClassificationModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class OutputModeration:\n",
    "    def __init__(self, toxic_keywords=None, repetition_threshold=3, max_output_length=500):\n",
    "        self.toxic_keywords = toxic_keywords or {\"hate\", \"violence\", \"malicious\", \"abuse\", \"harm\", \"terror\"}\n",
    "        self.repetition_threshold = repetition_threshold\n",
    "        self.max_output_length = max_output_length\n",
    "        self.previous_outputs = []\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # Load NeMo model for toxicity detection (this model is just an example)\n",
    "        self.toxicity_model = nemo_nlp.models.TextClassificationModel.from_pretrained('nvidia/bert-base-uncased')\n",
    "    \n",
    "    def hallucination_prevention(self, output: str, context: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the output aligns with the provided context to prevent hallucination.\n",
    "        \"\"\"\n",
    "        return context.lower() in output.lower()\n",
    "\n",
    "    def treat_as_untrusted(self, output: str) -> str:\n",
    "        \"\"\"\n",
    "        Treat output as untrusted and sanitize it for safe use.\n",
    "        \"\"\"\n",
    "        return html.escape(output)\n",
    "\n",
    "    def filter_toxicity(self, output: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect and filter toxic content using NeMo model.\n",
    "        \"\"\"\n",
    "        # Use NeMo model for toxicity detection (output is tokenized and predicted)\n",
    "        predictions = self.toxicity_model.predict([output])\n",
    "        # Assuming model predicts 0 = non-toxic, 1 = toxic\n",
    "        if predictions[0] == 1:  # Toxic content detected\n",
    "            return False\n",
    "        return True  # No toxicity detected\n",
    "\n",
    "    def anomaly_detection(self, output: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect anomalies in output using clustering (DBSCAN).\n",
    "        \"\"\"\n",
    "        words = output.split()\n",
    "        unique_words = set(words)\n",
    "\n",
    "        if len(unique_words) < self.repetition_threshold:\n",
    "            return False  # Repetition detected\n",
    "        \n",
    "        if len(output) > self.max_output_length:\n",
    "            return False  # Output too long\n",
    "\n",
    "        # Prepare data for clustering (for simplicity, use word counts)\n",
    "        self.previous_outputs.append(output)\n",
    "        word_counts = [len(output.split()) for output in self.previous_outputs]\n",
    "\n",
    "        # Standardize and cluster using DBSCAN\n",
    "        word_counts_scaled = self.scaler.fit_transform([[count] for count in word_counts])\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "        cluster_labels = dbscan.fit_predict(word_counts_scaled)\n",
    "        \n",
    "        # Anomaly if not belonging to a large cluster\n",
    "        if cluster_labels[-1] == -1:  # Outlier\n",
    "            return False  # Anomaly detected\n",
    "\n",
    "        return True  # No anomalies detected\n",
    "\n",
    "    def monitor_and_log(self, output: str, context: str, status: str):\n",
    "        \"\"\"\n",
    "        Monitor output and log for analysis and debugging, including timestamps, context, and status.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            with open(\"output_logs.txt\", \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"[{timestamp}] Context: {context}\\n\"\n",
    "                    f\"Output: {output}\\n\"\n",
    "                    f\"Status: {status}\\n\\n\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging output: {e}\")\n",
    "\n",
    "    def feedback_mechanism(self, output: str, is_acceptable: bool):\n",
    "        \"\"\"\n",
    "        Allow user feedback to refine future moderation decisions.\n",
    "        \"\"\"\n",
    "        feedback_file = \"moderation_feedback.txt\"\n",
    "        try:\n",
    "            with open(feedback_file, \"a\") as file:\n",
    "                feedback = \"Acceptable\" if is_acceptable else \"Unacceptable\"\n",
    "                file.write(f\"Output: {output}\\nFeedback: {feedback}\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving feedback: {e}\")\n",
    "\n",
    "\n",
    "class AsyncOutputProcessor:\n",
    "    def __init__(self, moderation_system: OutputModeration):\n",
    "        self.moderation_system = moderation_system\n",
    "\n",
    "    async def process_output_async(self, output: str, context: str):\n",
    "        \"\"\"\n",
    "        Asynchronously process output for moderation.\n",
    "        \"\"\"\n",
    "        # Treat output as untrusted and sanitize it\n",
    "        safe_output = self.moderation_system.treat_as_untrusted(output)\n",
    "        print(f\"Sanitized Output: {safe_output}\")\n",
    "\n",
    "        # Check for hallucinations\n",
    "        if not self.moderation_system.hallucination_prevention(safe_output, context):\n",
    "            status = \"Hallucinated or irrelevant content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "            return\n",
    "\n",
    "        # Filter for toxicity using NeMo model\n",
    "        if not self.moderation_system.filter_toxicity(safe_output):\n",
    "            status = \"Toxic or harmful content detected.\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "            self.moderation_system.feedback_mechanism(safe_output, False)\n",
    "            return\n",
    "\n",
    "        # Detect anomalies\n",
    "        if not self.moderation_system.anomaly_detection(safe_output):\n",
    "            status = \"Anomaly detected in content (e.g., repetition, length, nonsensical patterns).\"\n",
    "            print(f\"Warning: {status}\")\n",
    "            self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "            self.moderation_system.feedback_mechanism(safe_output, False)\n",
    "            return\n",
    "\n",
    "        # Log output for monitoring\n",
    "        status = \"Output accepted.\"\n",
    "        self.moderation_system.monitor_and_log(safe_output, context, status)\n",
    "        print(\"Output logged successfully.\\n\")\n",
    "\n",
    "        # Collect user feedback (mock example for demonstration)\n",
    "        self.moderation_system.feedback_mechanism(safe_output, is_acceptable=True)\n",
    "\n",
    "\n",
    "# Main Execution with Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the moderation system and processor\n",
    "    moderation_system = OutputModeration()\n",
    "    async_processor = AsyncOutputProcessor(moderation_system)\n",
    "\n",
    "    # Example context and outputs\n",
    "    context = \"Explain the concept of artificial intelligence.\"\n",
    "    outputs = [\n",
    "        \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "        \"I hate the concept of AI, it's promoting violence.\",\n",
    "        \"Hello Hello Hello\",\n",
    "        \"<script>alert('malicious');</script>\",\n",
    "        \"Artificial intelligence is a field that <b>abuses</b> resources.\",\n",
    "        \"AI can be a tool for both good and evil. However, there are concerns about misuse.\"\n",
    "    ]\n",
    "\n",
    "    # Use asynchronous processing for outputs\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [async_processor.process_output_async(output, context) for output in outputs]\n",
    "    loop.run_until_complete(asyncio.gather(*tasks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook provides a **foundational framework** for implementing output moderation in AI systems. The techniques demonstrated here—such as hallucination prevention, toxicity filtering, anomaly detection, and output sanitization—highlight the critical steps needed to ensure that AI-generated content is safe, relevant, and aligned with user expectations.\n",
    "\n",
    "It is important to note that this code serves as a **basic example**, emphasizing high-level principles rather than exhaustive implementation. Developers can use this as a starting point and expand upon it to address specific use cases, incorporate advanced moderation techniques, and integrate with production-level systems. \n",
    "\n",
    "Effective moderation is essential for building trust in AI applications, especially in sensitive or high-stakes domains. This notebook is a reminder of the key considerations that should guide the design of robust and ethical AI systems. Further refinements and domain-specific adjustments are encouraged to fully realize the potential of these principles in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
