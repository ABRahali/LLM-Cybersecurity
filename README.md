
# LLM-Cybersecurity ![Work in Progress](https://img.shields.io/badge/Status-Work%20in%20Progress-yellow)


# LLMs and Cybersecurity

This repository focuses on the intersection of Large Language Models (LLMs) and cybersecurity, addressing the risks of adversarial attacks and ensuring safe deployment in sensitive applications. It provides practical strategies, hands-on code, and research insights to secure LLMs and mitigate threats.

## Key Features

* **Adversarial Attacks** : Aims to analyze attack vectors like prompt injection, data poisoning, and exploitation techniques targeting LLMs.
* **Defense Strategies** : Plans to explore practical methods such as adversarial training and robust fine-tuning to defend LLMs.
* **Research Trends** : Seeks to summarize the latest research on LLM security and adversarial machine learning.
* **Model Safety** : Focuses on frameworks for ethical and secure LLM deployment, including risk mitigation and bias reduction.
* **State of the Art** : Plans to collect and discuss the latest advancements and challenges in securing large-scale model research papers.

This resource is dedicated to understanding and mitigating risks in LLMs, ensuring their secure and ethical deployment.

Below is a list of significant research papers from 2022 to 2024 that explore the use of LLMs for detecting various types of attacks:

| Year | Title                                                                                                                                                                                      | Focus                                                                                                  | Link                                                                                                                                    |
| ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------- |
| 2022 | [Eraser: Jailbreaking Defense in Large Language Models via Unlearning](https://arxiv.org/abs/2404.05880)                                                                                      | Investigates defense strategies for jailbreaking attacks on LLMs via unlearning mechanisms.            | [arxiv.org](https://arxiv.org/abs/2409.03274v3)                                                                                            |
| 2024 | [Defending Against Social Engineering Attacks in the Age of LLMs](https://aclanthology.org/2024.emnlp-main.716.pdf)                                                                           | Discusses strategies for defending against social engineering attacks using LLMs in cybersecurity.     | [aclanthology.org](https://aclanthology.org/2024.emnlp-main.716.pdf)                                                                       |
| 2024 | [Security Attacks on LLM-based Code Completion Tools](https://arxiv.org/abs/2408.11006v4)                                                                                                     | Examines vulnerabilities in LLM-based code completion tools and security issues like data extraction.  | [arxiv.org](https://arxiv.org/abs/2408.11006v4)                                                                                            |
| 2023 | [Research on Adversarial Attack and Defense of Large Language Models](https://www.researchgate.net/publication/385708844_Research_on_adversarial_attack_and_defense_of_large_language_models) | Focuses on adversarial attacks targeting LLMs and explores defense mechanisms to mitigate these risks. | [researchgate.net](https://www.researchgate.net/publication/385708844_Research_on_adversarial_attack_and_defense_of_large_language_models) |
| 2023 | [LLMmap: Fingerprinting For Large Language Models](https://arxiv.org/abs/2407.15847)                                                                                                          | Introduces a fingerprinting attack to identify and track LLMs based on their output patterns.          | [arxiv.org](https://arxiv.org/abs/2407.15847)                                                                                              |

Below is a list of significant research papers from 2022 to 2024 that explore prompt injection attacks on Large Language Models (LLMs):

| **Title**                                                                                                                                                                                                 | **Year** | **Link**                                                                                                 | **Summary**                                                                                                                                                                          |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Defending Against Prompt Injection with Preference Optimization](https://arxiv.org/abs/2410.05451)                                                                                                                | 2024           | [Link](https://arxiv.org/abs/2410.05451)                                                                          | Adapts prompt optimization methods to the prompt injection setting, enhancing LLM robustness against adversarial inputs. :contentReference[oaicite:0]{index=0}                             |
| [Text-Based Prompt Injection Attack Using Mathematical Functions](https://www.mdpi.com/2079-9292/13/24/5008)                                                                                                       | 2024           | [Link](https://www.mdpi.com/2079-9292/13/24/5008)                                                                 | Introduces a method to perform injection attacks by replacing sensitive words with mathematical functions, bypassing existing security policies. :contentReference[oaicite:1]{index=1}     |
| [Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods](https://openreview.net/pdf/feb18c5a7a896b819fcc59c646ca87161de12752.pdf)                                                                       | 2024           | [Link](https://openreview.net/pdf/feb18c5a7a896b819fcc59c646ca87161de12752.pdf)                                   | Studies backdoor-powered prompt injection attacks, highlighting their potential to nullify existing defense mechanisms. :contentReference[oaicite:2]{index=2}                              |
| [Defense Against Prompt Injection Attack by Leveraging Inverted Prompt Injection](https://arxiv.org/abs/2411.00459)                                                                                                | 2024           | [Link](https://arxiv.org/abs/2411.00459)                                                                          | Proposes novel defense methods against prompt injection attacks by inverting the attack process, enhancing LLM security. :contentReference[oaicite:3]{index=3}                             |
| [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://dl.acm.org/doi/10.1145/3658644.3690291)                                                                                                     | 2024           | [Link](https://dl.acm.org/doi/10.1145/3658644.3690291)                                                            | Introduces JudgeDeceiver, an optimization-based prompt injection attack targeting LLMs used as judges. :contentReference[oaicite:4]{index=4}                                               |
| [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://pubs.aip.org/aip/acp/article/3194/1/040013/3325235/Signed-prompt-A-new-approach-to-prevent-prompt) | 2024           | [Link](https://pubs.aip.org/aip/acp/article/3194/1/040013/3325235/Signed-prompt-A-new-approach-to-prevent-prompt) | Presents the 'Signed-Prompt' method to prevent prompt injection attacks by signing sensitive instructions, enabling LLMs to discern trusted sources. :contentReference[oaicite:5]{index=5} |
| [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)                                                                                                                    | 2023           | [Link](https://arxiv.org/abs/2312.17673)                                                                          | Introduces Jatmo, a method for generating task-specific models resilient to prompt-injection attacks through task-specific fine-tuning. :contentReference[oaicite:6]{index=6}              |
| [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                                                                                  | 2023           | [Link](https://arxiv.org/abs/2312.14197)                                                                          | Investigates indirect prompt injection attacks, developing benchmarks and defense methods to enhance LLM security. :contentReference[oaicite:7]{index=7}                                   |
| [Formalizing and Benchmarking Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2310.12815)                                                                                                             | 2023           | [Link](https://arxiv.org/abs/2310.12815)                                                                          | Provides a framework to formalize prompt injection attacks and defenses, offering a common benchmark for future research. :contentReference[oaicite:8]{index=8}                            |

Below is a list of notable research papers from 2022 to 2024 that explore the intersection of Large Language Models (LLMs) and cybersecurity:

| **Title**                                                                                                                                                                          | **Year** | **Link**                                                                                                            | **Summary**                                                                                                                                                                                                                                                                                                                                  |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [The Role of Machine Learning in Cybersecurity](https://arxiv.org/abs/2206.09707)                                                                                                           | 2022           | [Link](https://arxiv.org/abs/2206.09707)                                                                                     | Provides a comprehensive overview of how machine learning technologies, including LLMs, are applied within the cybersecurity domain. Discusses the advantages of machine learning over traditional methods and highlights challenges in deploying these technologies.                                                                              |
| [A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly](https://arxiv.org/abs/2312.02003)                                                            | 2023           | [Link](https://arxiv.org/abs/2312.02003)                                                                                     | Examines the dual aspects of LLMs in cybersecurity, detailing their beneficial applications, potential threats, and inherent vulnerabilities. Categorizes research into positive uses, offensive applications, and security concerns.                                                                                                              |
| [Large Language Models in Cybersecurity: State-of-the-Art](https://arxiv.org/abs/2402.00891)                                                                                                | 2023           | [Link](https://arxiv.org/abs/2402.00891)                                                                                     | Offers an in-depth analysis of both defensive and adversarial applications of LLMs within cybersecurity. Surveys current literature, identifies gaps, and evaluates risks and opportunities associated with LLM-driven cybersecurity.                                                                                                              |
| [When LLMs Meet Cybersecurity: A Systematic Literature Review](https://arxiv.org/abs/2405.03644)                                                                                            | 2023           | [Link](https://arxiv.org/abs/2405.03644)                                                                                     | Systematically investigates the application advancements of LLMs within the field of cybersecurity, covering over 180 academic papers since 2023. Provides a detailed overview of the current state, challenges, and future directions of LLM applications in cybersecurity.                                                                       |
| [Large Language Model (LLM) for Estimating the Cost of Cyber-attacks](https://www.researchgate.net/publication/383788930_Large_Language_Model_LLM_for_Estimating_the_Cost_of_Cyber-attacks) | 2023           | [Link](https://www.researchgate.net/publication/383788930_Large_Language_Model_LLM_for_Estimating_the_Cost_of_Cyber-attacks) | Proposes a framework utilizing Large Language Models (LLMs) and big data analytics to estimate the financial costs of cyber threats.                                                                                                                                                                                                               |
| [LLM for SoC Security: A Paradigm Shift](https://arxiv.org/abs/2310.06046)                                                                                                                  | 2023           | [Link](https://arxiv.org/abs/2310.06046)                                                                                     | Explores leveraging Generative Pre-trained Transformers (GPTs) to address gaps in System-on-Chip (SoC) security, aiming for a more efficient, scalable, and adaptable methodology.                                                                                                                                                                 |
| [Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia](https://arxiv.org/abs/2306.02612)                                                               | 2023           | [Link](https://arxiv.org/abs/2306.02612)                                                                                     | Investigates the potential role of LLMs in enhancing cybersecurity policies for Australian SMEs, providing a comprehensive understanding of their practical application, advantages, and limitations.                                                                                                                                              |
| [Generative AI and Large Language Models for Cyber Security: All Insights You Need](https://arxiv.org/abs/2405.12750)                                                                       | 2024           | [Link](https://arxiv.org/abs/2405.12750)                                                                                     | Provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). Explores LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. |
| [Large Language Models for Cyber Security: A Systematic Literature Review](https://arxiv.org/abs/2405.04760)                                                                                | 2024           | [Link](https://arxiv.org/abs/2405.04760)                                                                                     | Conducts a comprehensive review of the literature on the application of LLMs in cybersecurity. Identifies key findings, including the wide range of cybersecurity tasks LLMs are applied to, such as vulnerability detection, malware analysis, network intrusion detection, and phishing detection.                                               |
